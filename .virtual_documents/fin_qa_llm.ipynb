'''
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig

base_model_path = "meta-llama/Llama-3.2-3B"

quant_config = BitsAndBytesConfig(
    load_in_4bit=True,               # or load_in_8bit=True
    bnb_4bit_use_double_quant=True,  # only for 4bit
    bnb_4bit_quant_type="nf4",       # 'nf4' or 'fp4'
    bnb_4bit_compute_dtype=torch.bfloat16
    # bnb_8bit_compute_dtype=torch.float16 if load_in_8bit=True
)

tokenizer = AutoTokenizer.from_pretrained(base_model_path)

model = AutoModelForCausalLM.from_pretrained(
    base_model_path,
    quantization_config=quant_config,
    device_map="auto"  # Should place the model on your AMD GPU if recognized
)
'''


import torch
from transformers import AutoTokenizer, AutoModelForCausalLM

base_model_path = "meta-llama/Llama-3.2-3B"

# Load tokenizer
tokenizer = AutoTokenizer.from_pretrained(base_model_path)

# Load model for CPU
model = AutoModelForCausalLM.from_pretrained(
    base_model_path,
    device_map={"": "cpu"},       # Force CPU
    torch_dtype=torch.float32     # Use float32 (or float16 if CPU supports AVX512+BF16)
)
model.eval()

prompt = "Explain the concept of EBITDA"
inputs = tokenizer(prompt, return_tensors="pt")

with torch.no_grad():
    output_ids = model.generate(**inputs, max_new_tokens=100)
    
print(tokenizer.decode(output_ids[0], skip_special_tokens=True))


from peft import LoraConfig, get_peft_model

lora_config = LoraConfig(
    r=8,
    lora_alpha=16,
    lora_dropout=0.05,
    bias="none",
    task_type="CAUSAL_LM",
    target_modules=["q_proj", "v_proj"]
)

model = get_peft_model(model, lora_config)
print("LoRA model ready.")


import json
from datasets import load_dataset

raw_datasets = load_dataset("json", data_files="finance_qa.json", split="train")


def format_prompt(instruction, user_input, output=None):
    # You might want a template like below
    prompt = f"""Below is an instruction. Write a response.
### Instruction:
{instruction}

### Input:
{user_input}

### Response:
"""
    if output:
        prompt += output
    return prompt

def tokenize_example(example):
    instruction = example.get("instruction", "")
    input_text = example.get("input", "")
    answer_text = example.get("output", "")

    # Build the full text (prompt + answer)
    full_text = format_prompt(instruction, input_text, answer_text)
    tokenized = tokenizer(full_text, truncation=True, max_length=1024)
    # We'll set the labels to be the same as input_ids
    tokenized["labels"] = tokenized["input_ids"].copy()
    return tokenized

tokenized_datasets = raw_datasets.map(tokenize_example, batched=False)



from transformers import TrainingArguments, Trainer

training_args = TrainingArguments(
    output_dir="./llama3b-lora-checkpoint",
    per_device_train_batch_size=1,
    gradient_accumulation_steps=8,  # if you have small batch sizes
    num_train_epochs=3,
    learning_rate=2e-4,
    fp16=True,                    # with 8-bit weights, still often use fp16 for calculations
    eval_strategy="no",
    eval_steps=200,
    save_steps=200,
    logging_steps=50,
    report_to="none",            # or "wandb" if using Weights & Biases
)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_datasets,
    eval_dataset=None
)

trainer.train()



trainer.save_model()


from peft import PeftModel
from transformers import LlamaForCausalLM, LlamaTokenizer

base_model = LlamaForCausalLM.from_pretrained(
    base_model_path,
    load_in_8bit=True,
    device_map="auto"
)
lora_model = PeftModel.from_pretrained(base_model, "./llama3b-lora-checkpoint")

lora_model.eval()

# Test a prompt
prompt = """Below is an instruction. Write a response.
### Instruction:
Explain EBITDA in very simple terms for a business student.

### Input:
None

### Response:
"""
inputs = tokenizer(prompt, return_tensors="pt").to("cuda")
with torch.no_grad():
    generated_ids = lora_model.generate(**inputs, max_new_tokens=100)
output = tokenizer.decode(generated_ids[0], skip_special_tokens=True)
print(output)


import bitsandbytes as bnb
print(bnb.__version__)


export HIP_VISIBLE_DEVICES=0



